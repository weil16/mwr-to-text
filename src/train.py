# Standard library imports
import os
from pathlib import Path
from datetime import datetime

# Third-party imports
import numpy as np
import torch
import nltk
from torch.utils.data import DataLoader
from transformers import T5Tokenizer
from bert_score import score as bert_score
from tqdm import tqdm
import time
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    recall_score,
    roc_auc_score,
    confusion_matrix,
)


def train(args, config, MWRDataset, MWRModel, get_lora_config, load_and_preprocess, split_data):
    """Main training function"""
    # Setup directories
    os.makedirs(config["training"]["log_dir"], exist_ok=True)
    log_file = os.path.join(
        config["training"]["log_dir"],
        f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
    )

    # Device setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained(config["model"]["tokenizer_path"], legacy=True)
    lora_config = get_lora_config(config)
    model = MWRModel(config=config, lora_config=lora_config).to(device)

    # Optimizer and scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=config["training"]["learning_rate"])
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode="min",
        factor=config.get("training", {}).get("lr_factor", 0.5),
        patience=config.get("training", {}).get("lr_patience", 3),
    )

    current_epoch = 0

    # Load from checkpoint if specified
    if args.resume_from_checkpoint:
        checkpoint = torch.load(args.resume_from_checkpoint)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        current_epoch = checkpoint["epoch"]
        best_val_loss = checkpoint.get("best_val_loss", float("inf"))
        print(f"   å·²åŠ è½½æ¨¡å‹çŠ¶æ€ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€")

    # Data preparation
    if not all(
        os.path.exists(p)
        for p in [
            config["data"]["paths"]["train"],
            config["data"]["paths"]["val"],
            config["data"]["paths"]["test"],
        ]
    ):
        split_data()

    # Load and preprocess data
    train_df = load_and_preprocess(config["data"]["paths"]["train"])
    val_df = load_and_preprocess(config["data"]["paths"]["val"])

    # Create datasets
    dataset_config = {
        "max_length": config["data"]["preprocessing"]["max_seq_length"],
        "truncation_strategy": config["data"]["preprocessing"]["truncation_strategy"],
    }
    train_dataset = MWRDataset(train_df, tokenizer, **dataset_config)
    val_dataset = MWRDataset(val_df, tokenizer, **dataset_config)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["training"]["batch_size"],
        shuffle=True,
        num_workers=config["training"]["num_workers"],
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config["training"]["batch_size"],
        shuffle=False,
        num_workers=config["training"]["num_workers"],
    )

    # Training loop with early stopping
    best_val_loss = float("inf")
    total_epochs = config["training"]["epochs"]
    early_stopping_patience = config["training"]["early_stopping"]["patience"]
    patience_counter = 0

    print(f"\nå¼€å§‹è®­ç»ƒ - æ€»å…± {total_epochs} ä¸ªepoch")
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)} æ ·æœ¬")
    print(f"æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
    print(f"æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°: {len(train_loader)}")
    print("-" * 60)

    training_start_time = time.time()

    for epoch in range(current_epoch, total_epochs):
        model.train()
        total_loss = 0
        epoch_start_time = time.time()

        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            enumerate(train_loader),
            total=len(train_loader),
            desc=f"Epoch {current_epoch + epoch + 1}/{total_epochs - current_epoch}",
            ncols=100,
            leave=True,
        )

        for batch_idx, batch in pbar:
            try:
                # Move batch to device
                batch = {k: v.to(device) for k, v in batch.items()}

                # Forward pass
                outputs = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"],
                    class_labels=batch["class_labels"],
                )

                # Backward pass
                loss = outputs["loss"]
                loss.backward()

                # Gradient accumulation
                if (batch_idx + 1) % config["training"]["gradient_accumulation_steps"] == 0:
                    optimizer.step()
                    optimizer.zero_grad()

                total_loss += loss.item()

                # æ›´æ–°è¿›åº¦æ¡
                current_avg_loss = total_loss / (batch_idx + 1)
                pbar.set_postfix(
                    {
                        "Loss": f"{current_avg_loss:.4f}",
                        "Batch": f"{batch_idx + 1}/{len(train_loader)}",
                        "LR": f'{optimizer.param_groups[0]["lr"]:.2e}',
                    }
                )

            except Exception as e:
                print(f"\nè®­ç»ƒæ‰¹æ¬¡ {batch_idx + 1} å‘ç”Ÿé”™è¯¯: {e}")
                continue

        # å¤„ç†æœ€åä¸å®Œæ•´çš„æ¢¯åº¦ç´¯ç§¯æ‰¹æ¬¡
        if len(train_loader) % config["training"]["gradient_accumulation_steps"] != 0:
            optimizer.step()
            optimizer.zero_grad()

        # è®¡ç®—epochç»Ÿè®¡ä¿¡æ¯
        avg_train_loss = total_loss / len(train_loader)
        epoch_time = time.time() - epoch_start_time
        total_elapsed = time.time() - training_start_time

        # ä¼°ç®—å‰©ä½™æ—¶é—´
        avg_epoch_time = total_elapsed / (epoch + 1)
        remaining_epochs = total_epochs - (current_epoch + epoch + 1)
        estimated_remaining = avg_epoch_time * remaining_epochs

        # æ ¼å¼åŒ–æ—¶é—´
        def format_time(seconds):
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            seconds = int(seconds % 60)
            if hours > 0:
                return f"{hours}h {minutes}m {seconds}s"
            elif minutes > 0:
                return f"{minutes}m {seconds}s"
            else:
                return f"{seconds}s"

        # æ‰“å°è¯¦ç»†çš„epochæ€»ç»“
        print(f"\n Epoch {current_epoch + epoch + 1}/{total_epochs} å®Œæˆ!")
        print(f"   è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"   æœ¬epochç”¨æ—¶: {format_time(epoch_time)}")
        print(f"   æ€»ç”¨æ—¶: {format_time(total_elapsed)}")
        if remaining_epochs > 0:
            print(f"   é¢„è®¡å‰©ä½™æ—¶é—´: {format_time(estimated_remaining)}")

        # è®°å½•æ—¥å¿—
        log_message = f"Epoch {current_epoch + epoch + 1}/{total_epochs}, Train Loss: {avg_train_loss:.4f}, Time: {format_time(epoch_time)}, Total Time: {format_time(total_elapsed)}"
        with open(log_file, "a") as f:
            f.write(log_message + "\n")

        # Validation
        if (epoch + 1) % config["training"]["eval_interval"] == 0:
            print("\nğŸ” å¼€å§‹éªŒè¯...")
            eval_start_time = time.time()
            eval_results = evaluate(model, val_loader, device, tokenizer)
            eval_time = time.time() - eval_start_time

            print(f"   éªŒè¯æŸå¤±: {eval_results['loss']:.4f}")
            print(f"   åˆ†ç±»å‡†ç¡®ç‡: {eval_results['classification']['accuracy']:.4f}")
            print(f"   F1åˆ†æ•°: {eval_results['classification']['f1']:.4f}")
            print(f"   æ•æ„Ÿåº¦: {eval_results['classification']['sensitivity']:.4f}")
            print(f"   ç‰¹å¼‚åº¦: {eval_results['classification']['specificity']:.4f}")
            print(f"   AUC: {eval_results['classification']['auc']:.4f}")
            print(
                f"   BERTScore - P: {eval_results['bertscore']['precision']:.4f}, R: {eval_results['bertscore']['recall']:.4f}, F1: {eval_results['bertscore']['f1']:.4f}"
            )
            print(f"   METEOR Score: {eval_results['meteor']:.4f}")
            print(f"   éªŒè¯ç”¨æ—¶: {format_time(eval_time)}")

            # è®°å½•éªŒè¯æ—¥å¿—
            log_message = f"Epoch {epoch + 1}, Val Loss: {eval_results['loss']:.4f}, Val Time: {format_time(eval_time)}"
            log_message += f", Accuracy: {eval_results['classification']['accuracy']:.4f}"
            log_message += f", F1: {eval_results['classification']['f1']:.4f}"
            log_message += f", Sensitivity: {eval_results['classification']['sensitivity']:.4f}"
            log_message += f", Specificity: {eval_results['classification']['specificity']:.4f}"
            log_message += f", AUC: {eval_results['classification']['auc']:.4f}"
            log_message += f", BERTScore F1: {eval_results['bertscore']['f1']:.4f}"
            log_message += f", METEOR Score: {eval_results['meteor']:.4f}"
            with open(log_file, "a") as f:
                f.write(log_message + "\n")

            # Learning rate scheduling
            scheduler.step(eval_results["loss"])

            # Save best model and early stopping
            if eval_results["loss"] < best_val_loss:
                best_val_loss = eval_results["loss"]
                patience_counter = 0
                print(f"   ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! (éªŒè¯æŸå¤±: {best_val_loss:.4f})")
                save_model(
                    model,
                    config,
                    epoch,
                    optimizer,
                    scheduler,
                    best_val_loss,
                    "best_model",
                )
            else:
                patience_counter += 1
                print(f"   å½“å‰æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                print(f"   æ—©åœè®¡æ•°å™¨: {patience_counter}/{early_stopping_patience}")

                # Early stopping check
                if patience_counter >= early_stopping_patience:
                    print(f"\nâ¹ï¸ æ—©åœè§¦å‘! éªŒè¯æŸå¤±åœ¨ {early_stopping_patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„")
                    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                    break

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåœ¨éªŒè¯ä¹‹åï¼‰
        if (epoch + 1) % config["training"]["save_interval"] == 0:
            print(f"\nğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {config['training']['checkpoint_dir']}")
            save_model(model, config, epoch, optimizer, scheduler, best_val_loss, "checkpoint")

        print("-" * 60)

    # Return best validation loss for hyperparameter optimization
    return best_val_loss


def evaluate(model, data_loader, device, tokenizer):
    """Evaluate model on validation/test set with generation quality and classification metrics"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_refs = []
    all_class_preds = []
    all_class_labels = []
    batch_count = 0

    # åˆ›å»ºéªŒè¯è¿›åº¦æ¡
    eval_pbar = tqdm(data_loader, desc="éªŒè¯ä¸­", ncols=80, leave=False)

    with torch.no_grad():
        for batch in eval_pbar:
            try:
                batch = {k: v.to(device) for k, v in batch.items()}
                batch_count += 1

                # Forward pass
                outputs = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"],
                    class_labels=batch["class_labels"],
                )
                total_loss += outputs["loss"].item()

                # Get class predictions (assuming outputs contains logits or probabilities)
                class_preds = torch.argmax(outputs["class_logits"], dim=1).cpu().numpy()
                class_labels = batch["class_labels"].cpu().numpy()
                all_class_preds.extend(class_preds)
                all_class_labels.extend(class_labels)

                # Generate predictions
                generator = model.t5

                generated_ids = generator.generate(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    max_length=128,
                    num_beams=4,
                    early_stopping=True,
                )
                preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
                refs = tokenizer.batch_decode(batch["labels"], skip_special_tokens=True)
                all_preds.extend(preds)
                all_refs.extend(refs)

                # æ›´æ–°éªŒè¯è¿›åº¦æ¡ - ä¿®æ­£è®¡ç®—æ–¹å¼
                current_avg_loss = total_loss / batch_count
                eval_pbar.set_postfix({"Val Loss": f"{current_avg_loss:.4f}"})

            except Exception as e:
                print(f"\néªŒè¯æ‰¹æ¬¡å‘ç”Ÿé”™è¯¯: {e}")
                continue

    # Calculate metrics with strict filtering and detailed logging
    filtered_preds = []
    filtered_refs = []
    empty_samples = []

    for i, (pred, ref) in enumerate(zip(all_preds, all_refs)):
        # Strict filtering: non-empty strings with length > 0 after stripping
        if isinstance(pred, str) and isinstance(ref, str) and len(pred.strip()) > 0 and len(ref.strip()) > 0:
            filtered_preds.append(pred.strip())
            filtered_refs.append(ref.strip())
        else:
            empty_samples.append({"index": i, "prediction": pred, "reference": ref})

    # Log detailed filtering results
    if empty_samples:
        print(f"\n   âš ï¸ Filtered {len(empty_samples)} empty/invalid samples:")
        for sample in empty_samples[:5]:  # æœ€å¤šæ‰“å°5ä¸ªæ ·æœ¬é¿å…æ—¥å¿—è¿‡é•¿
            print(f"      Sample {sample['index']}:")
            print(f"        Input: {data_loader.dataset.input_texts[sample['index']]}")
            print(f"        Prediction: {repr(sample['prediction'])}")
            print(f"        Reference: {repr(sample['reference'])}")
        if len(empty_samples) > 5:
            print(f"      ... and {len(empty_samples)-5} more empty samples")

    metrics = {
        "loss": total_loss / len(data_loader),
        "text_generation": {
            "bertscore": None,
            "meteor": None,
        },
        "classification": {
            "accuracy": None,
            "f1": None,
            "sensitivity": None,
            "specificity": None,
            "auc": None,
        },
    }

    # Calculate classification metrics if we have valid samples
    if len(all_class_preds) > 0 and len(all_class_labels) > 0:
        try:
            # Calculate basic classification metrics
            metrics["classification"]["accuracy"] = accuracy_score(all_class_labels, all_class_preds)
            metrics["classification"]["f1"] = f1_score(all_class_labels, all_class_preds, average="weighted")
            metrics["classification"]["sensitivity"] = recall_score(
                all_class_labels, all_class_preds, average="weighted"
            )

            # Calculate specificity from confusion matrix
            cm = confusion_matrix(all_class_labels, all_class_preds)
            tn = cm.diagonal()
            fp = cm.sum(axis=0) - tn

            # é¿å…é™¤ä»¥é›¶ï¼šå½“åˆ†æ¯ä¸ºé›¶æ—¶ï¼Œå°†ç‰¹å¼‚æ€§è®¾ä¸º0æˆ–1ï¼ˆæ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼‰
            # è¿™é‡Œé€‰æ‹©è®¾ä¸º1ï¼Œè¡¨ç¤º"å®Œç¾é¢„æµ‹è´Ÿæ ·æœ¬"ï¼ˆå› ä¸ºæ²¡æœ‰è´Ÿæ ·æœ¬éœ€è¦é¢„æµ‹ï¼‰
            denominator = tn + fp
            specificity = np.zeros_like(tn, dtype=float)
            mask = denominator > 0
            specificity[mask] = tn[mask] / denominator[mask]
            specificity[~mask] = 1.0  # å¤„ç†åˆ†æ¯ä¸ºé›¶çš„æƒ…å†µ

            metrics["classification"]["specificity"] = np.mean(specificity)

            # Calculate AUC (handle binary and multiclass cases)
            try:
                if len(np.unique(all_class_labels)) == 2:  # Binary case
                    metrics["classification"]["auc"] = roc_auc_score(all_class_labels, all_class_preds)
                else:  # Multiclass case
                    metrics["classification"]["auc"] = roc_auc_score(
                        all_class_labels,
                        np.eye(np.max(all_class_labels) + 1)[all_class_preds],
                        multi_class="ovo",
                    )
            except Exception as e:
                print(f"   âš ï¸ AUC calculation error: {str(e)}")
                metrics["classification"]["auc"] = None

        except Exception as e:
            print(f"   âš ï¸ Classification metrics calculation error: {str(e)}")

    if len(filtered_preds) > 0 and len(filtered_refs) > 0:
        # Calculate BERTScore
        try:
            P, R, F1 = bert_score(
                filtered_preds,
                filtered_refs,
                lang="en",
                model_type="bert-base-uncased",
                device=device,
                verbose=False,  # Disable BERTScore internal warnings
            )
            metrics["bertscore"] = {
                "precision": P.mean().item(),
                "recall": R.mean().item(),
                "f1": F1.mean().item(),
            }
        except Exception as e:
            print(f"   âš ï¸ BERTScore calculation error: {str(e)}")

        # Calculate METEOR score
        try:
            meteor_scores = [
                nltk.translate.meteor_score.meteor_score([ref.split()], pred.split())
                for pred, ref in zip(filtered_preds, filtered_refs)
            ]
            metrics["meteor"] = sum(meteor_scores) / len(meteor_scores)
        except Exception as e:
            print(f"   âš ï¸ METEOR score calculation error: {str(e)}")
    else:
        print("   âš ï¸ No valid samples for metric calculations")

    return metrics


def save_model(model, config, epoch, optimizer, scheduler, best_val_loss, model_type="checkpoint"):
    """Save model checkpoint"""
    save_dir = (
        config["training"]["best_model_dir"] if model_type == "best_model" else config["training"]["checkpoint_dir"]
    )
    os.makedirs(save_dir, exist_ok=True)

    model_path = os.path.join(
        save_dir,
        ("best_model.pt" if model_type == "best_model" else f"{model_type}_epoch_{epoch + 1}.pt"),
    )

    torch.save(
        {
            "epoch": epoch + 1,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict(),
            "best_val_loss": best_val_loss,
            "config": config,
        },
        model_path,
    )
